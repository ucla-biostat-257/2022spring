{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Iterative-Methods-for-Solving-Linear-Equations\" data-toc-modified-id=\"Iterative-Methods-for-Solving-Linear-Equations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Iterative Methods for Solving Linear Equations</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#PageRank-problem\" data-toc-modified-id=\"PageRank-problem-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>PageRank problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Jacobi-method\" data-toc-modified-id=\"Jacobi-method-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Jacobi method</a></div><div class=\"lev2 toc-item\"><a href=\"#Gauss-Seidel-method\" data-toc-modified-id=\"Gauss-Seidel-method-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Gauss-Seidel method</a></div><div class=\"lev2 toc-item\"><a href=\"#Successive-over-relaxation-(SOR)\" data-toc-modified-id=\"Successive-over-relaxation-(SOR)-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Successive over-relaxation (SOR)</a></div><div class=\"lev2 toc-item\"><a href=\"#Conjugate-gradient-method\" data-toc-modified-id=\"Conjugate-gradient-method-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Conjugate gradient method</a></div><div class=\"lev2 toc-item\"><a href=\"#MatrixDepot.jl\" data-toc-modified-id=\"MatrixDepot.jl-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>MatrixDepot.jl</a></div><div class=\"lev2 toc-item\"><a href=\"#Numerical-examples\" data-toc-modified-id=\"Numerical-examples-18\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Numerical examples</a></div><div class=\"lev3 toc-item\"><a href=\"#Generate-test-matrix\" data-toc-modified-id=\"Generate-test-matrix-181\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Generate test matrix</a></div><div class=\"lev3 toc-item\"><a href=\"#Matrix-vector-muliplication\" data-toc-modified-id=\"Matrix-vector-muliplication-182\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Matrix-vector muliplication</a></div><div class=\"lev3 toc-item\"><a href=\"#Dense-solve-via-Cholesky\" data-toc-modified-id=\"Dense-solve-via-Cholesky-183\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Dense solve via Cholesky</a></div><div class=\"lev3 toc-item\"><a href=\"#Jacobi-solver\" data-toc-modified-id=\"Jacobi-solver-184\"><span class=\"toc-item-num\">1.8.4&nbsp;&nbsp;</span>Jacobi solver</a></div><div class=\"lev3 toc-item\"><a href=\"#Gauss-Seidal\" data-toc-modified-id=\"Gauss-Seidal-185\"><span class=\"toc-item-num\">1.8.5&nbsp;&nbsp;</span>Gauss-Seidal</a></div><div class=\"lev3 toc-item\"><a href=\"#SOR\" data-toc-modified-id=\"SOR-186\"><span class=\"toc-item-num\">1.8.6&nbsp;&nbsp;</span>SOR</a></div><div class=\"lev3 toc-item\"><a href=\"#Conjugate-Gradient\" data-toc-modified-id=\"Conjugate-Gradient-187\"><span class=\"toc-item-num\">1.8.7&nbsp;&nbsp;</span>Conjugate Gradient</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Methods for Solving Linear Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "So far we have considered direct methods for solving linear equations.    \n",
    "\n",
    "* **Direct methods** (flops fixed _a priori_) vs **iterative methods**:\n",
    "    - Direct method (GE/LU, Cholesky, QR, SVD): good for dense, small to moderate sized $\\mathbf{A}$  \n",
    "    - Iterative methods (Jacobi, Gauss-Seidal, SOR, conjugate-gradient, GMRES): good for large, sparse, structured linear system, parallel computing, warm start\n",
    "\n",
    "\n",
    "## PageRank problem\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/PageRanks-Example.svg/400px-PageRanks-Example.svg.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* $\\mathbf{A}  \\in \\{0,1\\}^{n \\times n}$ the connectivity matrix of webpages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij} = \\begin{cases}\n",
    "\t1 &  \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}. \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$n \\approx 10^9$ in May 2017.\n",
    "\n",
    "* $r_i = \\sum_j a_{ij}$ is the *out-degree* of page $i$. \n",
    "\n",
    "* [Larry Page](https://en.wikipedia.org/wiki/PageRank) imagines a random surfer wandering on internet according to following rules:\n",
    "    - From a page $i$ with $r_i>0$\n",
    "        * with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page  \n",
    "        * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page \n",
    "    - From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page\n",
    "    \n",
    "The process defines a Markov chain on the space of $n$ pages. Stationary distribution of this Markov chain gives the ranks (probabilities) of each page.\n",
    "\n",
    "* Stationary distribution is the top left eigenvector of the transition matrix $\\mathbf{P}$ corresponding to eigenvalue 1. Equivalently it can be cast as a linear equation\n",
    "$$\n",
    "    (\\mathbf{I} - \\mathbf{P}^T) \\mathbf{x} = \\mathbf{0}.\n",
    "$$\n",
    "\n",
    "* Gene Golub: Largest matrix computation in world. \n",
    "\n",
    "* GE/LU will take $2 \\times (10^9)^3/3/10^{12} \\approx 6.66 \\times 10^{14}$ seconds $\\approx 20$ million years on a tera-flop supercomputer!\n",
    "\n",
    "* Iterative methods come to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi method\n",
    "\n",
    "<img src=\"https://www.usna.edu/Users/math/meh/Jacobi.jpeg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "Solve $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "* Jacobi iteration: \n",
    "$$\n",
    "x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)}}{a_{ii}}.\n",
    "$$\n",
    "\n",
    "* With splitting: $\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$, Jacobi iteration can be written as\n",
    "$$\n",
    "    \\mathbf{D} \\mathbf{x}^{(t+1)} = - (\\mathbf{L} + \\mathbf{U}) \\mathbf{x}^{(t)} + \\mathbf{b},\n",
    "$$\n",
    "i.e., \n",
    "$$\n",
    "\t\\mathbf{x}^{(t+1)} = - \\mathbf{D}^{-1} (\\mathbf{L} + \\mathbf{U}) \\mathbf{x}^{(t)} + \\mathbf{D}^{-1} \\mathbf{b} = - \\mathbf{D}^{-1} \\mathbf{A} \\mathbf{x}^{(t)} + \\mathbf{x}^{(t)} + \\mathbf{D}^{-1} \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* One round costs $2n^2$ flops with an **unstructured** $\\mathbf{A}$. Gain over GE/LU if converges in $o(n)$ iterations. Saving is huge for **sparse** or **structured** $\\mathbf{A}$. By structured, we mean the matrix-vector multiplication $\\mathbf{A} \\mathbf{v}$ is fast ($O(n)$ or $O(n \\log n)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Seidel method\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "<img src=\"http://www.scientificlib.com/en/Physics/Biographies/images/Thumbs/ThLudwigVonSeidel01.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "* Gauss-Seidel (GS) iteration:\n",
    "$$\n",
    "x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)}}{a_{ii}}.\n",
    "$$\n",
    "\n",
    "* With splitting, $(\\mathbf{D} + \\mathbf{L}) \\mathbf{x}^{(t+1)} = - \\mathbf{U} \\mathbf{x}^{(t)} + \\mathbf{b}$, i.e., \n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = - (\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U} \\mathbf{x}^{(t)} + (\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* GS converges for any $\\mathbf{x}^{(0)}$ for symmetric and pd $\\mathbf{A}$.\n",
    "\n",
    "* Convergence rate of Gauss-Seidel is the spectral radius of the $(\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive over-relaxation (SOR)\n",
    "\n",
    "* SOR: \n",
    "$$\n",
    "x_i^{(t+1)} = \\omega \\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)} \\right) / a_{ii} + (1-\\omega) x_i^{(t)},\n",
    "$$\n",
    "i.e., \n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = (\\mathbf{D} + \\omega \\mathbf{L})^{-1} [(1-\\omega) \\mathbf{D} - \\omega \\mathbf{U}] \\mathbf{x}^{(t)} + (\\mathbf{D} + \\omega \\mathbf{L})^{-1} (\\mathbf{D} + \\mathbf{L})^{-1} \\omega \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* Need to pick $\\omega \\in [0,1]$ beforehand, with the goal of improving convergence rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "* **Conjugate gradient and its variants are the top-notch iterative methods for solving huge, structured linear systems.**\n",
    "\n",
    "* A UCLA invention! Hestenes and Stiefel in 60s.\n",
    "\n",
    "* Solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is equivalent to minimizing the quadratic function $\\frac{1}{2} \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$. \n",
    "\n",
    "[Kershaw's results](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub) for a fusion problem.\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky conjugate gradient | 25                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatrixDepot.jl\n",
    "\n",
    "MatrixDepot.jl is an extensive collection of test matrices in Julia. After installation, we can check available test matrices by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MatrixDepot\n",
    "\n",
    "mdinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List matrices that are positive definite and sparse:\n",
    "mdlist(:symmetric & :posdef & :sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get help on Poisson matrix\n",
    "mdinfo(\"poisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Poisson matrix of dimension n=10\n",
    "A = matrixdepot(\"poisson\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using UnicodePlots\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get help on Wathen matrix\n",
    "mdinfo(\"wathen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Wathen matrix of dimension n=5\n",
    "A = matrixdepot(\"wathen\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical examples\n",
    "\n",
    "The [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package implements most commonly used iterative solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, IterativeSolvers, LinearAlgebra, MatrixDepot, Random\n",
    "\n",
    "Random.seed!(280)\n",
    "\n",
    "n = 100\n",
    "# Poisson matrix of dimension n^2=10000, pd and sparse\n",
    "A = matrixdepot(\"poisson\", n)\n",
    "@show typeof(A)\n",
    "# dense matrix representation of A\n",
    "Afull = convert(Matrix, A)\n",
    "@show typeof(Afull)\n",
    "# sparsity level\n",
    "count(!iszero, A) / length(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage difference\n",
    "Base.summarysize(A), Base.summarysize(Afull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector muliplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generated vector of length n^2\n",
    "b = randn(n^2)\n",
    "# dense matrix-vector multiplication\n",
    "@benchmark $Afull * $b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix-vector multiplication\n",
    "@benchmark $A * $b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense solve via Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the Cholesky solution\n",
    "xchol = cholesky(Afull) \\ b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense solve via Cholesky\n",
    "@benchmark cholesky($Afull) \\ $b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi solver\n",
    "\n",
    "It seems that Jacobi solver doesn't give the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xjacobi = jacobi(A, b)\n",
    "@show norm(xjacobi - xchol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading [documentation](https://juliamath.github.io/IterativeSolvers.jl/dev/linear_systems/stationary/#Jacobi-1) we found that the default value of `maxiter` is 10. A couple trial runs shows that 30000 Jacobi iterations are enough to get an accurate solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xjacobi = jacobi(A, b, maxiter=30000)\n",
    "@show norm(xjacobi - xchol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark jacobi($A, $b, maxiter=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Seidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gauss-Seidel solution is fairly close to Cholesky solution after 15000 iters\n",
    "xgs = gauss_seidel(A, b, maxiter=15000)\n",
    "@show norm(xgs - xchol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark gauss_seidel($A, $b, maxiter=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetric SOR with Ï‰=0.75\n",
    "xsor = ssor(A, b, 0.75, maxiter=10000)\n",
    "@show norm(xsor - xchol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark sor($A, $b, 0.75, maxiter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conjugate gradient\n",
    "xcg = cg(A, b)\n",
    "@show norm(xcg - xchol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark cg($A, $b)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "143px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "443.5px",
    "left": "0px",
    "right": "796px",
    "top": "67px",
    "width": "164px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
