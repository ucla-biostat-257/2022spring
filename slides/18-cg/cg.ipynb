{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Krylov Subspace Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* Conjugate gradient is the top-notch iterative method for solving large, **structured** linear systems $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A}$ is pd.  \n",
    "Earlier we talked about Jacobi, Gauss-Seidel, and successive over-relaxation (SOR) as the classical iterative solvers. They are rarely used in practice due to slow convergence.  \n",
    "\n",
    "    [Kershaw's results](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub) for a fusion problem.\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky **conjugate gradient** | 25                   |\n",
    "\n",
    "\n",
    "* History: Hestenes (**UCLA** professor!) and Stiefel proposed conjugate gradient method in 1950s.\n",
    "\n",
    "Hestenes and Stiefel (1952), [Methods of conjugate gradients for solving linear systems](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf), _Jounral of Research of the National Bureau of Standards_.\n",
    "\n",
    "* Solve linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **pd**, is equivalent to \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\text{minimize} \\,\\, f(\\mathbf{x}) = \\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Denote $\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b} =: r(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient (CG) method\n",
    "\n",
    "* Consider a simple idea: coordinate descent, that is to update components $x_j$ alternatingly. Same as the Gauss-Seidel iteration. Usually it takes too many iterations.\n",
    "\n",
    "<img src=\"coordinate_descent.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* A set of vectors $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(l)}\\}$ is said to be **conjugate with respect to $\\mathbf{A}$** if\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}_i^T \\mathbf{A} \\mathbf{p}_j = 0, \\quad \\text{for all } i \\ne j.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "For example, eigen-vectors of $\\mathbf{A}$ are conjugate to each other. Why?\n",
    "\n",
    "* **Conjugate direction** method: Given a set of conjugate vectors $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(l)}\\}$, at iteration $t$, we search along the conjugate direction $\\mathbf{p}^{(t)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\alpha^{(t)} = - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "is the optimal step length.\n",
    "\n",
    "* Theorem: In conjugate direction method, $\\mathbf{x}^{(t)}$ converges to the solution in **at most** $n$ steps.\n",
    "\n",
    "    Intuition: Look at graph.\n",
    "    \n",
    "<img src=\"conjugate_direction.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* **Conjugate gradient** method. Idea: generate $\\mathbf{p}^{(t)}$ using only $\\mathbf{p}^{(t-1)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}^{(t)} = - \\mathbf{r}^{(t)} + \\beta^{(t)} \\mathbf{p}^{(t-1)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\beta^{(t)}$ is determined by the conjugacy condition $\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t)} = 0$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\beta^{(t)} = \\frac{\\mathbf{r}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}{\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **CG algorithm (preliminary version)**:  \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{A} \\mathbf{x}^{(t+1)} - \\mathbf{b}$\n",
    "        4. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{A} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        5. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        6. $t \\gets t+1$\n",
    "        \n",
    "    Remark: The initial conjugate direction $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$ is crucial.\n",
    "        \n",
    "* Theorem: With CG algorithm\n",
    "    0. $\\mathbf{r}^{(t)}$ are mutually orthogonal. \n",
    "    0. $\\{\\mathbf{r}^{(0)},\\ldots,\\mathbf{r}^{(t)}\\}$ is contained in the **Krylov subspace** of degree $t$ for $\\mathbf{r}^{(0)}$, denoted by\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        {\\cal K}(\\mathbf{r}^{(0)}; t) = \\text{span} \\{\\mathbf{r}^{(0)},\\mathbf{A} \\mathbf{r}^{(0)}, \\mathbf{A}^2 \\mathbf{r}^{(0)}, \\ldots, \\mathbf{A}^{t} \\mathbf{r}^{(0)}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    0. $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(t)}\\}$ is contained in ${\\cal K}(\\mathbf{r}^{(0)}; t)$. \n",
    "    0. $\\mathbf{p}^{(0)}, \\ldots, \\mathbf{p}^{(t)}$ are conjugate with respect to $\\mathbf{A}$.  \n",
    "The iterates $\\mathbf{x}^{(t)}$ converge to the solution in at most $n$ steps.\n",
    "\n",
    "* **CG algorithm (economical version)**: saves one matrix-vector multiplication.\n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        4. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{r}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        5. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        6. $t \\gets t+1$\n",
    "\n",
    "* Computation cost per iteration is **one** matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t)}$.  \n",
    "Consider PageRank problem, $\\mathbf{A}$ has dimension $n \\approx 10^{10}$ but is highly structured (sparse + low rank). Each matrix vector multiplication takes $O(n)$.\n",
    "    \n",
    "* Theorem: If $\\mathbf{A}$ has $r$ distinct eigenvalues, $\\mathbf{x}^{(t)}$ converges to solution $\\mathbf{x}^*$ in at most $r$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-conditioned conjugate gradient (PCG)\n",
    "\n",
    "* Summary of conjugate gradient method for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ or equivalently minimizing $\\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} -  \\mathbf{b}^T \\mathbf{x}$:\n",
    "    * Each iteration needs one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t+1)}$. For structured $\\mathbf{A}$, often $O(n)$ cost per iteration.\n",
    "    * Guaranteed to converge in $n$ steps.\n",
    "    \n",
    "* Two important bounds for conjugate gradient algorithm:\n",
    "\n",
    "    Let $\\lambda_1 \\le \\cdots \\le \\lambda_n$ be the ordered eigenvalues of a pd $\\mathbf{A}$.  \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& \\left( \\frac{\\lambda_{n-t} - \\lambda_1}{\\lambda_{n-t} + \\lambda_1} \\right)^2 \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 \\\\\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& 2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})}-1}{\\sqrt{\\kappa(\\mathbf{A})}+1} \\right)^{t} \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2,\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\kappa(\\mathbf{A}) = \\lambda_n/\\lambda_1$ is the condition number of $\\mathbf{A}$.\n",
    "\n",
    "<img src=\"cg_twocluster_spectrum.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src=\"cg_twocluster_iterates.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* Messages:\n",
    "    * Roughly speaking, if the eigenvalues of $\\mathbf{A}$ occur in $r$ distinct clusters, the CG iterates will _approximately_ solve the problem after $O(r)$ steps.  \n",
    "    * $\\mathbf{A}$ with a small condition number ($\\lambda_1 \\approx \\lambda_n$) converges fast.\n",
    "    \n",
    "* **Pre-conditioning**: Change of variables $\\widehat{\\mathbf{x}} = \\mathbf{C} \\mathbf{x}$ via a nonsingular $\\mathbf{C}$ and solve\n",
    "$$\n",
    "\t(\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}) \\widehat{\\mathbf{x}} = \\mathbf{C}^{-T} \\mathbf{b}.\n",
    "$$\n",
    "Choose $\\mathbf{C}$ such that \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has small condition number, or \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has clustered eigenvalues\n",
    "    * Inexpensive solution of $\\mathbf{C}^T \\mathbf{C} \\mathbf{y} = \\mathbf{r}$\n",
    "    \n",
    "* Preconditioned CG does not make use of $\\mathbf{C}$ explicitly, but rather the matrix $\\mathbf{M} = \\mathbf{C}^T \\mathbf{C}$.\n",
    "\n",
    "\n",
    "* **Preconditioned CG (PCG)** algorithm: \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$, pre-conditioner $\\mathbf{M}$\n",
    "    0. $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$\n",
    "    0. solve $\\mathbf{M} \\mathbf{y}^{(0)} = \\mathbf{r}^{(0)}$ for $\\mathbf{y}^{(0)}$\n",
    "    0. $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{y}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        4. Solve $\\mathbf{M} \\mathbf{y}^{(t+1)} \\gets \\mathbf{r}^{(t+1)}$ for $\\mathbf{y}^{(t+1)}$\n",
    "        5. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{y}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        6. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{y}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        7. $t \\gets t+1$\n",
    "\n",
    "    Remark: Only extra cost in the pre-conditioned CG algorithm is the need to solve the linear system $\\mathbf{M} \\mathbf{y} = \\mathbf{r}$.\n",
    "    \n",
    "* Pre-conditioning is more like an art than science. Some choices include     \n",
    "    * Incomplete Cholesky. $\\mathbf{A} \\approx \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^T$, where $\\tilde{\\mathbf{L}}$ is a sparse approximate Cholesky factor. Then $\\tilde{\\mathbf{L}}^{-1} \\mathbf{A} \\tilde{\\mathbf{L}}^{-T} \\approx \\mathbf{I}$ (perfectly conditioned) and $\\mathbf{M} \\mathbf{y} = \\tilde{\\mathbf{L}} \\tilde {\\mathbf{L}}^T \\mathbf{y} = \\mathbf{r}$ is easy to solve.  \n",
    "    * Banded pre-conditioners.  \n",
    "    * Choose $\\mathbf{M}$ as a coarsened version of $\\mathbf{A}$.\n",
    "    * Subject knowledge. Knowledge about the structure and origin of a problem is often the key to devising efficient pre-conditioner. For example, see recent work of Stein, Chen, Anitescu (2012) for pre-conditioning large covariance matrices. http://epubs.siam.org/doi/abs/10.1137/110834469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of PCG\n",
    "\n",
    "[Preconditioners.jl](https://github.com/mohamed82008/Preconditioners.jl) wraps a bunch of preconditioners.\n",
    "\n",
    "We use the Wathen matrix (sparse and positive definite) as a test matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, MatrixDepot, IterativeSolvers, LinearAlgebra, SparseArrays\n",
    "\n",
    "# Wathen matrix of dimension 30401 x 30401\n",
    "A = matrixdepot(\"wathen\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using UnicodePlots\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity level\n",
    "count(!iszero, A) / length(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rhs\n",
    "b = ones(size(A, 1))\n",
    "# solve Ax=b by CG\n",
    "xcg = cg(A, b);\n",
    "@benchmark cg($A, $b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the incomplete cholesky preconditioner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Preconditioners\n",
    "@time p = CholeskyPreconditioner(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-conditioned conjugate gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver Ax=b by PCG\n",
    "xpcg = cg(A, b, Pl=p)\n",
    "# same answer?\n",
    "norm(xcg - xpcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCG is >10 fold faster than CG\n",
    "@benchmark cg($A, $b, Pl=$p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems a regression of the either Preconditioners.jl or IterativeSolvers.jl package. We should see >10x speedup by using incomplete Cholesky preconditioner. Let's try the AMG preconditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AlgebraicMultigrid\n",
    "@time ml = ruge_stuben(A) # Construct a Ruge-Stuben solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use AMG preconditioner in CG\n",
    "pamg = aspreconditioner(ml)\n",
    "xamg = cg(A, b, Pl = pamg)\n",
    "# same answer?\n",
    "norm(xcg - xamg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark cg($A, $b, Pl = $pamg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Krylov subspace methods\n",
    "\n",
    "* We leant about CG/PCG, which is for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, $\\mathbf{A}$ pd.\n",
    "\n",
    "* **MINRES (minimum residual method)**: symmetric indefinite $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CG (bi-conjugate gradient)**: unsymmetric $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CGSTAB (Bi-CG stabilized)**: improved version of Bi-CG.\n",
    "\n",
    "* **GMRES (generalized minimum residual method)**: current _de facto_ method for unsymmetric $\\mathbf{A}$. E.g., PageRank problem.\n",
    "\n",
    "* **Lanczos method**: top eigen-pairs of a large symmetric matrix.\n",
    "\n",
    "* **Arnoldi method**: top eigen-pairs of a large unsymmetric matrix.\n",
    "\n",
    "* **Lanczos bidiagonalization** algorithm: top singular triplets of large matrix.\n",
    "\n",
    "* **LSQR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying CG to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I) \\beta = \\mathbf{X}^T \\mathbf{y}$.\n",
    "\n",
    "* **LSMR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying MINRES to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I) \\beta = \\mathbf{X}^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "### Matlab \n",
    "\n",
    "* Iterative methods for solving linear equations:  \n",
    "    `pcg`, `bicg`, `bicgstab`, `gmres`, ...\n",
    "* Iterative methods for top eigen-pairs and singular pairs:  \n",
    "    `eigs`, `svds`, ...\n",
    "* Pre-conditioner:  \n",
    "    `cholinc`, `luinc`, ...\n",
    "    \n",
    "* Get familiar with the **reverse communication interface (RCI)** for utilizing iterative solvers:\n",
    "```matlab\n",
    "x = gmres(A, b)\n",
    "x = gmres(@Afun, b)\n",
    "eigs(A)\n",
    "eigs(@Afun)\n",
    "```\n",
    "    \n",
    "### Julia\n",
    "\n",
    "* `eigs` and `svds` in the [Arpack.jl](https://github.com/JuliaLinearAlgebra/Arpack.jl) package. [Numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2019spring/slides/17-eigsvd/eigsvd.html#Lanczos/Arnoldi-iterative-method-for-top-eigen-pairs) later.\n",
    "\n",
    "* [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. [CG numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2019spring/slides/15-iterative/iterative.html#Numerical-examples)\n",
    "\n",
    "* Least squares examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, IterativeSolvers, LinearAlgebra, Random, SparseArrays\n",
    "\n",
    "Random.seed!(280) # seed\n",
    "n, p = 10000, 5000\n",
    "X = sprandn(n, p, 0.001) # iid standard normals with sparsity 0.01\n",
    "β = ones(p)\n",
    "y = X * β + randn(n)\n",
    "\n",
    "β̂_qr = X \\ y\n",
    "# least squares by QR\n",
    "@benchmark $X \\ $y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂_lsqr = lsqr(X, y)\n",
    "@show norm(β̂_qr - β̂_lsqr)\n",
    "# least squares by lsqr\n",
    "@benchmark lsqr($X, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂_lsmr = lsmr(X, y)\n",
    "@show norm(β̂_qr - β̂_lsmr)\n",
    "# least squares by lsmr\n",
    "@benchmark lsmr($X, $y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LinearMaps in iterative solvers\n",
    "\n",
    "In many applications, it is advantageous to define linear maps indead of forming the actual (sparse) matrix. For a linear map, we need to specify how it acts on right- and left-multiplication on a vector. The [`LinearMaps.jl`](https://github.com/Jutho/LinearMaps.jl) package is exactly for this purpose and interfaces nicely with `IterativeSolvers.jl`, `Arnoldi.jl` and other iterative solver packages.\n",
    "\n",
    "Applications:  \n",
    "1. The matrix is not sparse but admits special structure, e.g., easy + low rank (PageRank), Kronecker proudcts, etc.  \n",
    "2. Less memory usage. \n",
    "3. Linear algebra on a standardized (centered and scaled) sparse matrix.\n",
    "\n",
    "Consider the differencing operator that takes differences between neighboring pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearMaps, IterativeSolvers\n",
    "\n",
    "# Overwrite y with A * x\n",
    "# left difference assuming periodic boundary conditions\n",
    "function leftdiff!(y::AbstractVector, x::AbstractVector) \n",
    "    N = length(x)\n",
    "    length(y) == N || throw(DimensionMismatch())\n",
    "    @inbounds for i in 1:N\n",
    "        y[i] = x[i] - x[mod1(i - 1, N)]\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "# Overwrite y with A' * x\n",
    "# minus right difference\n",
    "function mrightdiff!(y::AbstractVector, x::AbstractVector) \n",
    "    N = length(x)\n",
    "    length(y) == N || throw(DimensionMismatch())\n",
    "    @inbounds for i in 1:N\n",
    "        y[i] = x[i] - x[mod1(i + 1, N)]\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "# define linear map\n",
    "D = LinearMap{Float64}(leftdiff!, mrightdiff!, 100; ismutating=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear maps can be used like a regular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show size(D)\n",
    "v = ones(size(D, 2)) # vector of all 1s\n",
    "@show D * v\n",
    "@show D' * v;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we form the corresponding dense matrix, it will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we form the corresponding sparse matrix, it will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "sparse(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using UnicodePlots\n",
    "spy(sparse(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute top singular values using iterative method (Arnoldi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Arpack\n",
    "Arpack.svds(D, nsv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "# check solution against the direct method for SVD\n",
    "svdvals(Matrix(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute top eigenvalues of the Gram matrix `D'D` using iterative method (Arnoldi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arpack.eigs(D'D, nev=3, which=:LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Chapter 5 of [Numerical Optimization](https://ucla.worldcat.org/title/numerical-optimization/oclc/209918411&referer=brief_results) by Jorge Nocedal and Stephen Wright (1999).\n",
    "\n",
    "* Sections 11.3-11.5 of [Matrix Computations](https://ucla.worldcat.org/title/matrix-computations/oclc/824733531&referer=brief_results) by Gene Golub and Charles Van Loan (2013)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "443.5px",
    "left": "0px",
    "right": "795px",
    "top": "67px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
